{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We all know that BERT model and it's variants (DISTILBERT and ROBERTa) can process a maximum sequence length of 512. The sequences which are longer than the max sequence length are truncated and the ones less than 512 can be padded up to a fixed length to maintain sequence length uniformity throughout the dataset.\n",
        "\n",
        "We'll look how to pass long text sequences to BERT and it's variants, without needing to truncate them, as some sequences may have relevant information after the 512 token length limit.\n",
        "\n",
        "We'll use the DistilBERT model as it is much lighter and has less trainable parameters, which will use less computation resources and space."
      ],
      "metadata": {
        "id": "10D2hLIU3_Iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VLghS4dxBHE"
      },
      "outputs": [],
      "source": [
        "### Install libraries\n",
        "!pip install transformers\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the 20NewsGroup dataset available on sklearn.dataset class"
      ],
      "metadata": {
        "id": "JX86KhApnxRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ],
      "metadata": {
        "id": "deLaBymxy8Vc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a dataframe of columns text and labels\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "df = pd.DataFrame(list(zip(newsgroups_train.data,newsgroups_train.target)),columns=['text','label'])\n",
        "df = df.sample(frac=1).head(5000)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8ODPMExjRVI",
        "outputId": "faa2f6aa-cabf-4a91-e82d-8f66b607399a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                    text  label\n",
            "4028   From: mlee@eng.sdsu.edu (Mike Lee)\\nSubject: M...      1\n",
            "4686   From: rclark@nyx.cs.du.edu\\nSubject: Re: Is th...     11\n",
            "10440  From: kkeller@mail.sas.upenn.edu (Keith Keller...     10\n",
            "7219   From: bobbe@vice.ICO.TEK.COM (Robert Beauchain...      0\n",
            "6618   From: emm@tamarack202.cray.com (Mike McConnell...      1\n",
            "...                                                  ...    ...\n",
            "10654  From: mantolov@golum.riv.csu.edu.au (Michael A...      4\n",
            "10417  From: sforsblo@vipunen.hut.fi (Svante Forsblom...     10\n",
            "10032  From: steveg@cadkey.com (Steve Gallichio)\\nSub...     10\n",
            "9322   From: James_Jim_Frazier@cup.portal.com\\nSubjec...      4\n",
            "4642   From: (Joseph D. Barrus)\\nSubject: Utility to ...      2\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label2ids = {label:i for i,label in enumerate(newsgroups_train.target_names)}\n",
        "ids2label = {id:label for label,id in label2ids.items()}\n",
        "print(label2ids)\n",
        "print(ids2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azl27Pf6kXlH",
        "outputId": "85944c58-a737-4973-da53-178cfbe36622"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alt.atheism': 0, 'comp.graphics': 1, 'comp.os.ms-windows.misc': 2, 'comp.sys.ibm.pc.hardware': 3, 'comp.sys.mac.hardware': 4, 'comp.windows.x': 5, 'misc.forsale': 6, 'rec.autos': 7, 'rec.motorcycles': 8, 'rec.sport.baseball': 9, 'rec.sport.hockey': 10, 'sci.crypt': 11, 'sci.electronics': 12, 'sci.med': 13, 'sci.space': 14, 'soc.religion.christian': 15, 'talk.politics.guns': 16, 'talk.politics.mideast': 17, 'talk.politics.misc': 18, 'talk.religion.misc': 19}\n",
            "{0: 'alt.atheism', 1: 'comp.graphics', 2: 'comp.os.ms-windows.misc', 3: 'comp.sys.ibm.pc.hardware', 4: 'comp.sys.mac.hardware', 5: 'comp.windows.x', 6: 'misc.forsale', 7: 'rec.autos', 8: 'rec.motorcycles', 9: 'rec.sport.baseball', 10: 'rec.sport.hockey', 11: 'sci.crypt', 12: 'sci.electronics', 13: 'sci.med', 14: 'sci.space', 15: 'soc.religion.christian', 16: 'talk.politics.guns', 17: 'talk.politics.mideast', 18: 'talk.politics.misc', 19: 'talk.religion.misc'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['len'] = df.text.apply(lambda t: len(t.split()))\n",
        "df.len.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8te9EZmrVuq",
        "outputId": "f4bc28f8-2f5e-4a3c-95d9-d3b8c4e05586"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     5000.000000\n",
              "mean       291.394800\n",
              "std        548.426855\n",
              "min         15.000000\n",
              "25%        109.000000\n",
              "50%        180.000000\n",
              "75%        295.000000\n",
              "max      11263.000000\n",
              "Name: len, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Basic Preprocessing\n",
        "def text_preprocessing(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('\\n',' ').replace('\\r',' ').replace('\\t',' ')\n",
        "  text = re.sub(r\" {2,}\",\" \",text)\n",
        "  return text\n",
        "\n",
        "df['text'] = df.text.apply(lambda t: text_preprocessing(t))"
      ],
      "metadata": {
        "id": "Vt3mC14Sjb84"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['encoded_label'] = df['label'].astype('category').cat.codes\n",
        "NUM_LABELS = len(df['encoded_label'].value_counts().index)\n",
        "print(NUM_LABELS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbanMrkj0Rc",
        "outputId": "8955fc6e-9767-408c-840a-2ae91beb8ccd"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label2ids = dict(zip(df.label,df.encoded_label))\n",
        "# print(label2ids)"
      ],
      "metadata": {
        "id": "4qD5PG-Vj0zS"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts,val_texts,train_labels,val_labels = train_test_split(df.text.tolist(),df.encoded_label.tolist(),test_size=0.2,random_state=42)\n",
        "print(type(train_texts),type(train_labels))\n",
        "print(len(train_texts),len(val_texts),len(train_labels),len(val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHBKKcrxkFgJ",
        "outputId": "9e6948a5-a629-4e6c-88f7-c7661166ccd3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> <class 'list'>\n",
            "4000 1000 4000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "mj-71ch1bNld"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch"
      ],
      "metadata": {
        "id": "eFXG8oaXvlam"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=NUM_LABELS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP4FJ78ekm4H",
        "outputId": "c8e0230f-8209-41a6-cbc0-d65748b6e912"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = tokenizer(train_texts)\n",
        "test_tokens = tokenizer(val_texts)\n",
        "train_encodings = [{\"input_ids\":ids, \"attention_mask\":mask} for ids,mask in zip(train_tokens['input_ids'],train_tokens['attention_mask'])]\n",
        "test_encodings = [{\"input_ids\":ids, \"attention_mask\":mask} for ids,mask in zip(test_tokens['input_ids'],test_tokens['attention_mask'])]"
      ],
      "metadata": {
        "id": "WfZO-knbqzyg"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sliding Window Class will attend to all the data without truncating. The input_id sequences more than 512 sequence will be splitted by length of window size. Each subsequent windows will have the overlap of the tokens from the previous token.\n",
        "Arguments:\n",
        "* window_size, default 256, length of each window slide. Max value should be 2 less than the no of max length defined in the transformer model architecture.\n",
        "* overlap, default 128, length of the overlap of tokens from the previous slide.\n",
        "* tokenizer, default 'distilbert', required to set the START END bit token values. Other available tokenizer = ['roberta']\n",
        "* pad_sequence_flag, default True, sets padding to the windows\n",
        "* MAX_LEN, default 512, BERT based models can attend maximum 512 sequence lengths, other architecture can attend as defined in their paper."
      ],
      "metadata": {
        "id": "uebQ17AlDuvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SlidingWindow:\n",
        "\n",
        "  def __init__(self, window_size=256, overlap=128, tokenizer='distilbert', pad_sequence_flag=True, MAX_LEN=512):\n",
        "    self.window_size = window_size\n",
        "    self.overlap = overlap\n",
        "    self.tokenizer = tokenizer\n",
        "    self.pad_sequence_flag = pad_sequence_flag\n",
        "    self.pad_sequence_length = MAX_LEN\n",
        "\n",
        "  def pad_sequences(self,sequence,padding_type='POST',sequence_type='input_ids',tokenizer='distilbert'):\n",
        "    if self.tokenizer == 'distilbert':\n",
        "      if sequence_type == 'input_ids':\n",
        "        padding_data = [0]\n",
        "      elif sequence_type == 'attention_mask':\n",
        "        padding_data = [0]\n",
        "    elif self.tokenizer == 'roberta':\n",
        "      if sequence_type == 'input_ids':\n",
        "        padding_data = [1]\n",
        "      elif sequence_type == 'attention_mask':\n",
        "        padding_data = [0]\n",
        "\n",
        "    if padding_type == 'POST':\n",
        "      sequence.extend(padding_data*(self.pad_sequence_length-len(sequence)))\n",
        "      return sequence\n",
        "    elif padding_type == 'PRE':\n",
        "      temp_data = [padding_data]*(self.pad_sequence_length-len(sequence))\n",
        "      return temp_data\n",
        "\n",
        "  def create_token_windows(self,tokenizer_sequence,labels,label_ids_flag=False,label_ids_list=[]):\n",
        "    if label_ids_flag == True and (len(label_ids_flag) != len(tokenizer_sequence['input_ids'])):\n",
        "      print(\"Either flag of label_ids_flag is not set to True or label_ids_list is empty or length not equal to the length of tokenizer_sequence\")\n",
        "      sys.exit(1)\n",
        "\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    if self.tokenizer == \"roberta\":\n",
        "      start_token = [0]\n",
        "      end_token = [2]\n",
        "    elif self.tokenizer == \"distilbert\":\n",
        "      start_token = [101]\n",
        "      end_token = [102]\n",
        "    else:\n",
        "      print(\"[ERROR] No Tokenizer Defined - \", self.tokenizer)\n",
        "      sys.exit()\n",
        "\n",
        "    for index,input_sequence in enumerate(tokenizer_sequence):\n",
        "      sequence = input_sequence['input_ids'][1:-1]\n",
        "      attention_sequence = input_sequence['attention_mask'][1:-1]\n",
        "      label = labels[index]\n",
        "\n",
        "      for i in range(0,len(sequence),self.window_size-self.overlap):\n",
        "        window_input_ids = start_token + sequence[i:i+self.window_size] + end_token\n",
        "        window_input_ids = self.pad_sequences(window_input_ids,sequence_type=\"input_ids\",tokenizer=self.tokenizer)\n",
        "        input_ids_list.append(window_input_ids)\n",
        "\n",
        "        attention_mask = [1]+attention_sequence[i:i+self.window_size]+[1]\n",
        "        attention_mask = self.pad_sequences(attention_mask,sequence_type=\"attention_mask\",tokenizer=self.tokenizer)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "\n",
        "        if label_ids_flag == True:\n",
        "          labels_list.append(label_ids_list[label])\n",
        "        else:\n",
        "          labels_list.append(label)\n",
        "\n",
        "    return input_ids_list,attention_mask_list,labels_list\n"
      ],
      "metadata": {
        "id": "YCc06kWRkreT"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sw = SlidingWindow(window_size=510, overlap=128, tokenizer='distilbert')"
      ],
      "metadata": {
        "id": "Ss4uLXx7mG_5"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids,train_masks,train_labels = sw.create_token_windows(train_encodings,train_labels)"
      ],
      "metadata": {
        "id": "-cmg-prPm32n"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids,test_masks,test_labels = sw.create_token_windows(test_encodings,val_labels)"
      ],
      "metadata": {
        "id": "AzjgPin9naAU"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids = torch.tensor(train_input_ids,dtype=torch.long)\n",
        "train_masks = torch.tensor(train_masks,dtype=torch.long)\n",
        "train_labels = torch.tensor(train_labels,dtype=torch.long)"
      ],
      "metadata": {
        "id": "YXRnLfYx-8eo"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids = torch.tensor(test_input_ids,dtype=torch.long)\n",
        "test_masks = torch.tensor(test_masks,dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels,dtype=torch.long)"
      ],
      "metadata": {
        "id": "GUVlKUR2DD5h"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZKgyIimHnNm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}